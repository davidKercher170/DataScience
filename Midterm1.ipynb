{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import csv\n",
    "from scipy.optimize import minimize, least_squares"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Baby Boomer Divorce Rates\n",
    "\n",
    "The file babyboomerdivorce.csv contains data on the rate of baby-boomer divorce. The problem has two independent variables: the marriage age and years of school, denoted by x and y respectively.\n",
    "\n",
    "a. Formulate a linear regression learning problem for each of the following buckets of models.\n",
    "\n",
    "$\\\\$ Model 1 : $A = \\begin{bmatrix} \n",
    "\t1 & x_1 & y_1 \\\\\n",
    "\t1 & x_2 & y_2 \\\\\n",
    "    .. & .. & .. \\\\\n",
    "\t1 & x_n & y_n \\\\\n",
    "\t\\end{bmatrix}$, $\\theta = [\\theta_{00}, \\theta_{10}, \\theta_{01}]^T$\n",
    "\n",
    "$argmin_{\\theta}||A\\theta - b||_2$\n",
    "\n",
    "$\\\\$ Model 2 : $A = \\begin{bmatrix} \n",
    "\t1 & x_1 & y_1 & x_1y_1 \\\\\n",
    "\t1 & x_2 & y_2 & x_2y_2 \\\\\n",
    "    .. & .. & .. \\\\\n",
    "\t1 & x_n & y_n & x_ny_n \\\\\n",
    "\t\\end{bmatrix}$ $\\theta = [\\theta_{00}, \\theta_{10}, \\theta_{01}, \\theta_{11}]^T$\n",
    "\n",
    "$argmin_{\\theta}||A\\theta - b||_2$\n",
    "\n",
    "$\\\\$ Model 3 : $A = \\begin{bmatrix} \n",
    "\t1 & x_1 & y_1 & x_1y_1 & x_1^2 & y_1^2\\\\\n",
    "\t1 & x_2 & y_2 & x_2y_2 & x_2^2 & y_2^2\\\\\n",
    "    .. & .. & .. \\\\\n",
    "\t1 & x_n & y_n & x_ny_n & x_n^2 & y_n^2 \\\\\n",
    "\t\\end{bmatrix}$, $\\theta = [\\theta_{00}, \\theta_{10}, \\theta_{01}, \\theta_{11}, \\theta_{20}, \\theta_{02}]^T$\n",
    "\n",
    "$argmin_{\\theta}||A\\theta - b||_2$\n",
    "\n",
    "b. Solve the learning problems that you formulated in part (a) and report he best fit parameter values and uncertainties in each parameter.\n",
    "\n",
    "$\\\\$ The best fit parameters were within the third model (the full results posted in the code below). $\\theta = $[ 1.14478739e+02 -2.17977168e+00 -3.05970381e-02  1.00666766e-01\n",
    " -1.87273229e-02 -1.67386364e-01]. Right away it is troubling that the first theta value, $\\theta_{00}$, is so much larger than the other parameters.\n",
    "\n",
    "c. Calculate and report the leave-one-out cross validation error for each bucket of models in part (a). Which bucket performs best according to this metric?\n",
    "\n",
    "$\\\\$ However calculating the leave-one-out cross validation error the first model, Model 1 with only 3 parameters, had the best Cost Value.\n",
    "\n",
    "d. Formulate a linear regression problem for the bucket of models using the 1-norm. Solve the problem using a convex optimization solver and report the best fit parameter values.\n",
    "\n",
    "$\\\\$ We can use the same formulations for the models as posted above, however now with $theta^* = argmin_{\\theta}||A\\theta - b||_1$. Then we can use scipy to calculate the theta values that obtain a minimum. The best fit parameters for Model 1 were $\\theta = $[126.73699162  -1.84671145  -2.30199812]."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1. Cost: 19.679551485957365 ---- Theta: [122.18308858  -1.88084719  -1.94914286]\n",
      "Model 2. Cost: 15.836753427794445 ---- Theta: [ 1.67870700e+02 -3.34051530e+00 -5.10001262e+00  1.00666766e-01]\n",
      "Model 3. Cost: 13.855672523516391 ---- Theta: [ 1.14478739e+02 -2.17977168e+00 -3.05970381e-02  1.00666766e-01\n",
      " -1.87273229e-02 -1.67386364e-01]\n"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "    with open(\"f1.csv\") as f:\n",
    "        c = csv.reader(f)\n",
    "        x = []\n",
    "        y = []\n",
    "        b = []\n",
    "        for row in c:\n",
    "            try:\n",
    "                x.append(float(row[0]))\n",
    "                y.append(float(row[1]))\n",
    "                b.append(float(row[2]))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return x,y,b\n",
    "\n",
    "x,y,b = get_data()\n",
    "b = np.array(b)\n",
    "\n",
    "m = len(x)\n",
    "\n",
    "def A_matrix(x,y,m, model=1):\n",
    "    if model==1:\n",
    "        return np.array([[1, x[i], y[i]] for i in range(m)])\n",
    "    if model==2:\n",
    "        return np.array([[1, x[i], y[i], x[i]*y[i]] for i in range(m)])\n",
    "    if model==3:\n",
    "        return np.array([[1, x[i], y[i], x[i]*y[i], x[i]**2,  y[i]**2] for i in range(m)])\n",
    "\n",
    "\n",
    "parameters = []\n",
    "\n",
    "for model_num in range(1,4):\n",
    "    A = A_matrix(x,y,m, model=model_num)\n",
    "    c = np.linalg.lstsq(A,b,rcond=None)\n",
    "    error = np.linalg.norm(b-A@c[0], ord=2)\n",
    "    print(\"Model \" + str(model_num) + \". Cost: \" + str(error) + \" ---- Theta: \" + str(c[0]))\n",
    "    parameters.append(c[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model 1. Error: 58.27586600227191\n",
      "Model 2. Error: 97.56606357524058\n",
      "Model 3. Error: 202.1598284890386\n"
     ]
    }
   ],
   "source": [
    "errors = []\n",
    "\n",
    "# for set of parameters learned from the bucket of models\n",
    "for model_num in range(1,4):\n",
    "    A = A_matrix(x,y,m, model=model_num)\n",
    "    err = 0\n",
    "    for j in range(m):\n",
    "        A_new = np.copy(A)\n",
    "        b_new = np.copy(b)\n",
    "        for k in range(len(A[0])):\n",
    "            A_new[j][k] = 0\n",
    "        b_new[j] = 0\n",
    "        c = np.linalg.lstsq(A,b_new,rcond=None)[0]\n",
    "        err += (b[j] - (A[j]@c))**2\n",
    "    errors.append(err/m)\n",
    "\n",
    "for model_num in range(1,4):\n",
    "    print(\"Model \" + str(model_num) + \". Error: \" + str(errors[model_num-1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Cost: 66.42747385921943 ---- Theta: [126.73699162  -1.84671145  -2.30199812]\n"
     ]
    }
   ],
   "source": [
    "def get_data():\n",
    "    with open(\"f1.csv\") as f:\n",
    "        c = csv.reader(f)\n",
    "        x = []\n",
    "        y = []\n",
    "        b = []\n",
    "        for row in c:\n",
    "            try:\n",
    "                x.append(float(row[0]))\n",
    "                y.append(float(row[1]))\n",
    "                b.append(float(row[2]))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return x,y,b\n",
    "\n",
    "x,y,b = get_data()\n",
    "b = np.array(b)\n",
    "m = len(x)\n",
    "\n",
    "def cost(c):\n",
    "    out = A_matrix(x,y,m)@c - b\n",
    "    return np.linalg.norm(out, ord=1)\n",
    "\n",
    "v = minimize(cost, [1,1,1])\n",
    "theta = v[\"x\"]\n",
    "print(\"Cost: \" + str(cost(theta)) + \" ---- Theta: \" + str(theta))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Analysis of an Enzyme Reaction\n",
    "\n",
    "The file salmonella.csv contains data for the activity of a colony of salmonella as a function of temperature (in\n",
    "Celsius). These data should be fit to the following model:\n",
    "\n",
    "$$f_{\\theta}(T) = \\frac{S_Be^{-E_B/T}}{S_Ae^{-E_A/T} + S_Be^{-E_B/T} + S_Ce^{-E_C/T}}$$ \n",
    "\n",
    "a. Show that this bucket of models has two structurally unidentifiable parameters by rewriting it in terms of the following identifiable combinations:\n",
    "\n",
    "First we can simplify this model by eliminating two parameters. We can do this by multiplying the top and bottom of the equation by $S_A e^{E_A/T}$ eliminating one term with parameters from the denominator. This gives us:\n",
    "\n",
    "$$f_{\\theta}(T) = \\frac{S_Ae^{E_A/T}}{S_Ae^{E_A/T}}\\frac{S_Be^{-[E_B-E_A]/T}}{1 + \\frac{S_B}{S_A}e^{-[E_B-E_A]/T} + \\frac{S_C}{S_A}e^{-[E_C-E_A]/T}}$$ \n",
    "\n",
    "Thus now instead of our parameters being $\\theta = (S_A, E_A, S_B, E_B, S_C, E_C)'$ our parameters are now $\\theta = (\\frac{S_B}{S_A}, E_B-E_A,\\frac{S_C}{S_A}, E_C-E_A)'$. Meaning that we had two structurally unidentifiable parameters. (Theta now only consists of 4 parameters rather than 6)\n",
    "\n",
    "b. Fit the 4-parameter model that you found in part (a) and plot the fit against the data. This model can be challenging to fit, so use the visualizations in part (b) to guide your initial parameter guess. (Hint 1: Be sure to convert your temperatures to Kelvin. Hint 2: Each of the parameters in part (a) should be positive. I found it easier to find a good fit when using the log of the parameter values.\n",
    "\n",
    "#### I fit the model using the 1, 2, and inf norms. The results (as printed in the code box below were):\n",
    "\n",
    "1 Norm. Cost: 3.235645430375456 ---- $\\theta = $ [ 5.59630525e-03  4.90431008e+01 -3.63701102e-01 -3.18694045e+00]\n",
    "\n",
    "2 Norm. Cost: 0.41037943460984144 ---- $\\theta = $[ 4.94778972e-06 -2.93127612e+02  1.48224929e-04 -3.75514224e+02]\n",
    "\n",
    "Inf Norm. Cost: 0.32866120134337834 ----$\\theta = $[4.00099523e-03 6.29808162e+00 1.06749999e+00 6.36034742e+00]\n",
    "\n",
    "####  Additionally using the hint given that the parameters should all be positive, the results were:\n",
    "\n",
    "1 Norm. Cost: 7.6183763820743104 ---- $\\theta = $ [0.00611732 1.2085457  2.57618796 0.92051712]\n",
    "\n",
    "2 Norm. Cost: 0.8328369308500786 ---- $\\theta = $ [  0.87965523  31.10739387 174.9768124    0.        ]\n",
    "\n",
    "Inf Norm. Cost: 0.3748165141454243 ---- $\\theta = $ [0.00602286 1.11448707 2.70400066 0.92174118]\n",
    "\n",
    "c. Calculate the Fisher Information Matrix for your model at the best fit and report your estimated uncertainty in each parameter. Be sure to include the factor of σ in the FIM.\n",
    "\n",
    "The Fisher Information Matrix is found by multiplying the Jacobian matrix by the Jacobian Transpose. The Jacobian was calulated by finding the derivative relative to each term $\\theta_i$ represtend in the ith column of the matrix and in each row evaluating the derivatives for each value T.\n",
    "\n",
    "( 2.58502709e+00  9.38464382e-04 -4.16096087e-03  1.03707321e-02\n",
    "\n",
    " 9.38464382e-04  4.48282270e-07 -1.50571622e-06  3.75283018e-06\n",
    "\n",
    " -4.16096087e-03 -1.50571622e-06  6.69786632e-06 -1.66936867e-05\n",
    "\n",
    " 1.03707321e-02  3.75283018e-06 -1.66936867e-05  4.16071571e-05 )\n",
    "\n",
    " Next, to estimate parametric uncertainty we can find the product of $\\sigma^2 * (A'A)^{-1} = FIM(\\theta)$. In general the parametric uncertainty appears to be low.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Without Bounds: \n",
      "2 Norm. Cost: 0.41037943460984144 ---- Theta: [ 4.94778972e-06 -2.93127612e+02  1.48224929e-04 -3.75514224e+02]\n",
      "1 Norm. Cost: 3.235645430375456 ---- Theta: [ 5.59630525e-03  4.90431008e+01 -3.63701102e-01 -3.18694045e+00]\n",
      "Inf Norm. Cost: 0.32866120134337834 ---- Theta: [4.00099523e-03 6.29808162e+00 1.06749999e+00 6.36034742e+00]\n",
      "\n",
      "With Bounds: \n",
      "2 Norm. Cost: 0.8328369308500786 ---- Theta: [  0.87965523  31.10739387 174.9768124    0.        ]\n",
      "1 Norm. Cost: 7.6183763820743104 ---- Theta: [0.00611732 1.2085457  2.57618796 0.92051712]\n",
      "Inf Norm. Cost: 0.3748165141454243 ---- Theta: [0.00602286 1.11448707 2.70400066 0.92174118]\n"
     ]
    }
   ],
   "source": [
    "# get the data\n",
    "def get_data():\n",
    "    with open(\"f2.csv\") as f:\n",
    "        c = csv.reader(f)\n",
    "        x = []\n",
    "        b = []\n",
    "        for row in c:\n",
    "            try:\n",
    "                x.append(float(row[0]))\n",
    "                b.append(float(row[1]))\n",
    "            except Exception:\n",
    "                continue\n",
    "    return x,b\n",
    "\n",
    "# read and formalize as a numpy array\n",
    "T_vals,b = get_data()\n",
    "b = np.array(b)\n",
    "\n",
    "\n",
    "# uses the formula derived from part b. Multiplies by 255.927778* to convert from Fahrenheit to Kelvin\n",
    "def f(theta):\n",
    "    x = [255.927778*theta[0]*np.exp(-theta[1]/T)/(1 + theta[0]*np.exp(-theta[1]/T) + theta[2]*np.exp(-theta[3]/T)) for T in T_vals]\n",
    "    return np.linalg.norm([x[i] - b[i] for i in range(len(b))], ord=2)\n",
    "\n",
    "def g(theta):\n",
    "    x = [255.927778*theta[0]*np.exp(-theta[1]/T)/(1 + theta[0]*np.exp(-theta[1]/T) + theta[2]*np.exp(-theta[3]/T)) for T in T_vals]\n",
    "    return np.linalg.norm([x[i] - b[i] for i in range(len(b))], ord=1)\n",
    "\n",
    "def h(theta):\n",
    "    x = [255.927778*theta[0]*np.exp(-theta[1]/T)/(1 + theta[0]*np.exp(-theta[1]/T) + theta[2]*np.exp(-theta[3]/T)) for T in T_vals]\n",
    "    return np.linalg.norm([x[i] - b[i] for i in range(len(b))], ord=np.Inf)\n",
    "\n",
    "\n",
    "print(\"Without Bounds: \")\n",
    "theta = minimize(f,[1,1,1,1])\n",
    "print(\"2 Norm. Cost: \" + str(f(theta[\"x\"])) + \" ---- Theta: \" + str(theta[\"x\"]))\n",
    "\n",
    "theta = minimize(g,[1,1,1,1])\n",
    "print(\"1 Norm. Cost: \" + str(g(theta[\"x\"])) + \" ---- Theta: \" + str(theta[\"x\"]))\n",
    "\n",
    "theta = minimize(h,[1,1,1,1])\n",
    "print(\"Inf Norm. Cost: \" + str(h(theta[\"x\"])) + \" ---- Theta: \" + str(theta[\"x\"]))\n",
    "\n",
    "print()\n",
    "print(\"With Bounds: \")\n",
    "\n",
    "bnds = ((0, None), (0, None), (0, None), (0, None))\n",
    "theta = minimize(f,[1,1,1,1], bounds=bnds)\n",
    "print(\"2 Norm. Cost: \" + str(f(theta[\"x\"])) + \" ---- Theta: \" + str(theta[\"x\"]))\n",
    "\n",
    "theta = minimize(g,[1,1,1,1], bounds=bnds)\n",
    "print(\"1 Norm. Cost: \" + str(g(theta[\"x\"])) + \" ---- Theta: \" + str(theta[\"x\"]))\n",
    "\n",
    "theta = minimize(h,[1,1,1,1], bounds=bnds)\n",
    "print(\"Inf Norm. Cost: \" + str(h(theta[\"x\"])) + \" ---- Theta: \" + str(theta[\"x\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 2.58502709e+00  9.38464382e-04 -4.16096087e-03  1.03707321e-02]\n",
      " [ 9.38464382e-04  4.48282270e-07 -1.50571622e-06  3.75283018e-06]\n",
      " [-4.16096087e-03 -1.50571622e-06  6.69786632e-06 -1.66936867e-05]\n",
      " [ 1.03707321e-02  3.75283018e-06 -1.66936867e-05  4.16071571e-05]]\n"
     ]
    }
   ],
   "source": [
    "# 2 Part C - Calculate the FIM and report the estimated uncertainty in each parameter.\n",
    "# The Fisher Information Matrix is found by multiplying the Jacobian by the Jacobian transpose\n",
    "\n",
    "# Jacobian Matrix\n",
    "def jacob(theta):\n",
    "    J = np.zeros((len(b),4))\n",
    "\n",
    "    for i in range(len(b)):\n",
    "        T = T_vals[i]\n",
    "        Top = theta[0]*np.exp(-theta[1]/T)\n",
    "        Bottom = (1 + theta[0]*np.exp(-theta[1]/T) + theta[2]*np.exp(-theta[3]/T))\n",
    "\n",
    "        # the derivatives with respect to theta_1, theta_2, theta_3, theta_4 evalutated at each T value T_i (so each function derivative below is in column i)\n",
    "        J[i][0] = (np.exp(-theta[1]/T)*Bottom - theta[0]*np.exp(-2*theta[1]/T)) / Bottom**2\n",
    "        J[i][1] = (theta[1]/T*theta[0]*np.exp(-theta[1]/T)*Bottom - theta[1]**2*-theta[1]/T*Top ) / Bottom**2\n",
    "        J[i][2] = -1*Top/Bottom**2*(np.exp(-theta[3]/T))\n",
    "        J[i][3] = Top/Bottom**2*theta[2]*theta[3]*(np.exp(-theta[3]/T))\n",
    "\n",
    "    return J\n",
    "\n",
    "# Fisher Information Matrix\n",
    "def FIM(theta):\n",
    "    j = jacob(theta)\n",
    "    return j.T @ j\n",
    "\n",
    "print(FIM([0.00602286, 1.11448707, 2.70400066, 0.92174118]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Two-Norm Penalty for Weights\n",
    "\n",
    "Derive a close-form expression for the solution to the following variation on the typical least-squares problem. where A, b and $\\lambda$ are given, but no rank or dimension assumptions on A can be guaranteed. Hint: Try to reduce this problem to the standard least squares problem for a new matrix $\\hat{A}$ and a vector $\\hat{v}$ and then apply a solution to this problem.\n",
    "\n",
    "We can find the closed form solution by finding an explicit formula for $(||Ax-b||_2^2 + \\lambda ||x||_2^2)$, taking the derivative and setting the expression equal to 0. Let, \n",
    "\n",
    "$$f(x) = (||Ax-b||_2^2 + \\lambda ||x||_2^2)$$\n",
    "\n",
    "First from HW 2 we know that $$||Ax-b||_2^2 = \\sum_i^m [ \\sum_j^n \\sum_k^n A_{ij}x_jA_{ik}x_k-2b_i \\sum_j^n (A_{ij}x_j)+b_i^2]$$\n",
    "\n",
    "Additionally $\\lambda ||x||_2^2 = \\lambda \\sum_i^n x_i^2$. Thus combinding the two expressions we have,\n",
    "\n",
    "$$f(x) = \\sum_i^m [ \\sum_j^n \\sum_k^n A_{ij}x_jA_{ik}x_k-2b_i \\sum_j^n (A_{ij}x_j)+b_i^2] + \\lambda \\sum_i^n x_i^2$$\n",
    "\n",
    "Then taking the derivative of the function we have:\n",
    "\n",
    "$$\\frac{df}{dx_{\\mu}} = 2\\sum_i^n (A_{i \\mu} \\sum A_{ij}x_j - b_iA_{i\\mu}) + 2 \\lambda x_{\\mu}$$\n",
    "\n",
    "Which we can rewrite as:\n",
    "\n",
    "$$\\frac{df}{dx} = 2[A'Ax - A'b + \\lambda x]$$\n",
    "\n",
    "Then in order to find the minimum value we set the derivative $\\frac{df}{dx} = 2[A'Ax - A'b + \\lambda x] = 0$. \n",
    "\n",
    "$$\\frac{df}{dx} = 2[A'Ax - A'b + \\lambda x] = 0$$\n",
    "\n",
    "$$ \\implies A'Ax - A'b + \\lambda x = 0$$\n",
    "\n",
    "$$ \\implies A'Ax + \\lambda x = A'b$$\n",
    "\n",
    "$$ \\implies x(A'A + \\lambda) = A'b$$\n",
    "\n",
    "$$ \\implies x^* = (A'A + \\lambda)^{-1}A'b$$\n",
    "\n",
    "Lastly if desired we can check that this is a minimum by checking that the second derivative is positive. However this follows directly from previous work from HW 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Part 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Overdetermined Linear Regression is Biased\n",
    "\n",
    "Consider a linear least squares regression model. Assume the errors are normally distributed according to e ~ $N(0, \\sigma^2 I)$ and that there are more parameters than data points so that A is a fat matrix. Show that the estimator $\\hat{\\theta} = A^+ y$ is biased. \n",
    "\n",
    "First Note that the pseudo-inverse $A^+$ is defined by $A^+ = (A'A)^{-1}A'$. Additionally we know that the bias of the esimator is defined by $bias(\\hat{\\theta}) = <\\hat{\\theta}> - \\theta$.\n",
    "\n",
    "Then, \n",
    "\n",
    "$$bias(\\theta, \\hat{\\theta}) = <\\hat{\\theta}> - \\theta$$\n",
    "$$ = <A^+y> - \\theta$$\n",
    "$$ = <A^+(A\\theta + e)> - \\theta$$\n",
    "$$ = <A^+A\\theta + A^+e> - \\theta$$\n",
    "\n",
    "By the definition of pseudo-inverse $A^+A = I$.\n",
    "\n",
    "\n",
    "$$ \\implies  bias(\\theta, \\hat{\\theta}) = <\\theta + A^+e> - \\theta$$\n",
    "\n",
    "The expectation <> is a linear operator, thus we can distribute through addition. Thus this becomes,\n",
    "\n",
    "$$ = <\\theta> + <A^+e> - \\theta$$\n",
    "$$ = \\theta + <A^+e> - \\theta$$\n",
    "$$ = <A^+e> \\neq 0$$\n",
    "\n",
    "Thus $bias(\\theta, \\hat{\\theta}) \\neq 0$ and so by definition the model is biased.\n",
    "\n",
    "$\\\\$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5 Poisson Distribution from the Binomial Distribution\n",
    "\n",
    "The binomial distribution is a two-parameter family of distributions given by:\n",
    "\n",
    "$$ P(m;n,p) = (n \\ m)p^m(1-p)^{n-m}$$\n",
    "\n",
    "If you have a coin that has probability p of coming up heads and you flip it n times, then P gives the probability of getting m heads. In this two-parameter bucket of models the parameters satisify $p \\in [0,]$ and $n \\in N$.\n",
    "\n",
    "\n",
    "Models of probability distributions, such as this, have regimes of practically unidentificable parameters just as nonlinear regression models do. Show that in the regime $p \\to 0$ and $n \\to \\infty$, the parameter $\\lambda = np$ is the identifiable combination. Whas is an expression for the probability distribution in this regime? (Hint: Define $\\lambda = np$, replace $p=\\lambda/n$ and take the limit $n \\to \\infty$.)\n",
    "\n",
    "Admittedly from the title of the problem, I was first inspired to try to prove that this Distribution will become a Poisson Distribution. The formula for a Poisson Distribution is given by,\n",
    "\n",
    "$$Pois(\\lambda, m) =  \\frac{\\lambda^m e^{-m}}{m!}$$\n",
    "\n",
    "First we will note that the choose function $(n \\ m) = \\frac{n!}{m!(n-m)!}$.\n",
    "\n",
    "As recommended in the hint, we will set $\\lambda = np$ and $p = \\lambda / n$ so that we can more simply just take the limit as $n \\to \\infty$.\n",
    "\n",
    "Then, \n",
    "\n",
    "$$lim_{n \\to \\infty} P(m;n,p) =  lim_{n \\to \\infty} P(m;n,\\lambda/n)$$ \n",
    "\n",
    "$$ = lim_{n \\to \\infty} \\frac{n!}{m!(n-m)!} (\\frac{\\lambda}{n})^m (1-\\frac{\\lambda}{n})^{n-m}$$\n",
    "\n",
    "In order to match the Poisson Distribution, we will first take the $\\frac{1}{m!}$ and $\\lambda^m$ terms out to the outside of the limit,\n",
    "\n",
    "$$ =\\frac{\\lambda^m}{m!} lim_{n \\to \\infty} \\frac{n!}{(n-m)!} (\\frac{1}{n})^m (1-\\frac{\\lambda}{n})^{n-m}$$\n",
    "\n",
    "Next we can distribute the limit operator such that,\n",
    "\n",
    "$$ \\frac{\\lambda^m}{m!} lim_{n \\to \\infty} \\frac{n!}{(n-m)!} (\\frac{1}{n})^m (1-\\frac{\\lambda}{n})^{n-m} = \\frac{\\lambda^m}{m!} lim_{n \\to \\infty} [\\frac{n!}{(n-m)!} (\\frac{1}{n})^m ] lim_{n \\to \\infty} [(1-\\frac{\\lambda}{n})^{n-m}]$$\n",
    "\n",
    "$$ = \\frac{\\lambda^m}{m!} lim_{n \\to \\infty} (1-\\frac{\\lambda}{n})^{n-m}$$\n",
    "\n",
    "Since $lim_{n \\to \\infty} \\frac{n!}{(n-m)!} (\\frac{\\lambda}{n})^m = lim_{n \\to \\infty} \\frac{\\lambda^m n!}{n(n-m)!} = lim_{n \\to \\infty} \\frac{\\lambda^m (n-1)!}{(n-m)!} = \\lambda^m$.\n",
    "\n",
    "Lastly the limit,\n",
    "\n",
    "$$lim_{n \\to \\infty} (1-\\frac{\\lambda}{n})^{n-m} = lim_{n \\to \\infty} (1-\\frac{\\lambda}{n})^{n} \\ lim_{n \\to \\infty} (1-\\frac{\\lambda}{n})^{-m}$$\n",
    "\n",
    "$$= lim_{n \\to \\infty} (1+\\frac{1}{n})^{-\\lambda n} \\ lim_{n \\to \\infty} (1+\\frac{1}{n})^{\\lambda m}$$\n",
    "\n",
    "$$= e^{-\\lambda} \\ (1)^{\\lambda m} = e^{-\\lambda}$$\n",
    "\n",
    "Thus when put all together we have,\n",
    "\n",
    "$$ lim_{n \\to \\infty} P(m;n,p) = \\frac{\\lambda^m e^{-\\lambda}}{m!} = Pois(\\lambda, m)$$\n",
    "\n",
    "Thus the Binomial Distribution becomes a Poisson Distribution with the regime $p \\to 0$ and $n \\to \\infty$.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f058019491b5a987a146e2cbdf9c3254eb932582dd8a90e9955176c2995d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
