{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Problem 1\n",
      "\n",
      "1-Norm\n",
      "0:  1-Norm: 769.594 --- 2-Norm: 256.614\n",
      "1:  1-Norm: 98.87 --- 2-Norm: 35.775\n",
      "2:  1-Norm: 26.2 --- 2-Norm: 11.835\n",
      "3:  1-Norm: 25.894 --- 2-Norm: 12.211\n",
      "4:  1-Norm: 22.851 --- 2-Norm: 9.859\n",
      "5:  1-Norm: 21.291 --- 2-Norm: 10.208\n",
      "6:  1-Norm: 21.164 --- 2-Norm: 10.934\n",
      "7:  1-Norm: 10.689 --- 2-Norm: 6.597\n",
      "8:  1-Norm: 8.476 --- 2-Norm: 5.935\n",
      "9:  1-Norm: 7.638 --- 2-Norm: 5.41\n",
      "10:  1-Norm: 1.634 --- 2-Norm: 1.634\n",
      "\n",
      "Inf-Norm\n",
      "0:  inf-Norm: 116.376 --- 2-Norm: 259.892\n",
      "1:  inf-Norm: 15.547 --- 2-Norm: 34.865\n",
      "2:  inf-Norm: 4.247 --- 2-Norm: 11.039\n",
      "3:  inf-Norm: 4.223 --- 2-Norm: 11.437\n",
      "4:  inf-Norm: 3.741 --- 2-Norm: 10.447\n",
      "5:  inf-Norm: 3.078 --- 2-Norm: 9.471\n",
      "6:  inf-Norm: 2.953 --- 2-Norm: 9.369\n",
      "7:  inf-Norm: 2.249 --- 2-Norm: 7.224\n",
      "8:  inf-Norm: 1.797 --- 2-Norm: 5.731\n",
      "9:  inf-Norm: 1.455 --- 2-Norm: 4.607\n",
      "10:  inf-Norm: 0.424 --- 2-Norm: 1.403\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cvxpy as cp\n",
    "import math\n",
    "from scipy.optimize import fmin\n",
    "\n",
    "times = [1900, 1910, 1920,1930,1940,1950,1960,1970,1980,1990,2000,2010]\n",
    "times = [i for i in range(12)]\n",
    "b = [75.995, 91.972, 105.711, 123.203, 131.669, 150.697, 179.323, 203.212, 226.505, \n",
    "249.633, 281.422, 308.746]\n",
    "\n",
    "times = np.array(times)\n",
    "b = np.array(b)\n",
    "\n",
    "print(\"-------\")\n",
    "print(\"Problem 1\")\n",
    "print()\n",
    "\n",
    "def A(T,N):\n",
    "    A = [[t**i for i in range(N)] for t in T]\n",
    "    return np.array(A)\n",
    "\n",
    "print(\"1-Norm\")\n",
    "for n in range(1,12):\n",
    "    B = A(times,n)\n",
    "    x = cp.Variable(n)\n",
    "    \n",
    "    cost = cp.norm(B@x - b, p=1)\n",
    "    prob = cp.Problem(cp.Minimize(cost))\n",
    "    prob.solve()\n",
    "\n",
    "    print(str(n-1) + \":  1-Norm: \" + str(round(cp.norm(B@x - b,p=1).value,3)) + \" --- 2-Norm: \" + str(round(cp.norm(B@x - b,p=2).value,3)))\n",
    "\n",
    "print()\n",
    "print(\"Inf-Norm\")\n",
    "for n in range(1,12):\n",
    "    B = A(times,n)\n",
    "    x = cp.Variable(n)\n",
    "    \n",
    "    cost = cp.norm(B@x - b, p=\"inf\")\n",
    "    prob = cp.Problem(cp.Minimize(cost))\n",
    "    prob.solve()\n",
    "\n",
    "    print(str(n-1) + \":  inf-Norm: \" + str(round(cp.norm(B@x - b,p=\"inf\").value,3)) + \" --- 2-Norm: \" + str(round(cp.norm(B@x - b,p=2).value,3)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Description (for the work above): The 1 and infinity norms follow the 2-norm error closely. The lowest error was at degree 10 (which is to be expected) for all 3 norms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Part C: Leave-One-Out Error\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1e4aadf88b0>]"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXoAAAD4CAYAAADiry33AAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAAAdVElEQVR4nO3deXxU9b3G8c+3ICAKYhUEA5FFrCK7I4girVVaRFtwaUWv9dZqkdsiQbGtdrda29pF0HKlqLR1qXGjFRFF29oqVJAga4hoAIVAFBAIshPyvX/M4B3ikJwkM5mZM8/79ZoXmTnL/H4mPjk5z5wZc3dERCS8PpXuAYiISGop6EVEQk5BLyIScgp6EZGQU9CLiIRc03QPIJHjjz/eO3funO5hiIhkjYULF25297aJlmVk0Hfu3JmioqJ0D0NEJGuY2XuHW6ZTNyIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iEnIJeRCQDLHxvC3/496qU7FtBLyKSRpUHqpj497f56h/m8Zc31rJzb2XSnyMjr4wVEckF67bsYvwTi1n43lYu7ZfH7SNO56jmyY9lBb2ISBo8u3g9P/zrcgAmjerLiL55KXsuBb2ISCPavmc/P3m2mL8uWk/kpGO554q+dPp0y5Q+p4JeRKSRLHxvCwWFiymv2MNNF5zCt8/rRtMmqa9KFfQiIilWeaCK379Syn3/LOXENi148oZBnHHSsY32/Ap6EZEUWrdlFzc9sZiiuMK1VYsjGnUMCnoRkRRpzMK1Jgp6EZEk+2jPfn7cyIVrTRT0IiJJtPC9rYx/YhEbtjVu4VqTQM9uZsPMbKWZlZrZrQmWn2pmr5vZXjO7Je7xTmb2ipmVmFmxmRUkc/AiIpmi8kAVk/7+Dl/9w+sAPHnDIAou6J72kIcAR/Rm1gSYDAwFyoAFZjbD3VfErbYFGAeMrLZ5JTDB3d80s1bAQjN7udq2IiJZLb5wvaRfHj9LQ+FakyCnbgYApe6+GsDMCoERwMdh7e4bgY1mdlH8hu5eDpTHvv7IzEqAvPhtRUSyWaYUrjUJEvR5wLq4+2XAwLo+kZl1BvoB8w+zfDQwGiA/P7+uuxcRaVTxhesZJx3LxDQXrjUJEvSW4DGvy5OY2dHAM8B4d9+eaB13nwpMBYhEInXav4hIYzpYuK7fupvxF3Rn7HknZ8S5+MMJEvRlQKe4+x2BDUGfwMyOIBryj7n79LoNT0Qkc1QeqGLyK6u495/v0OGYFjw1ZhBnnPTpdA+rVkGCfgHQ3cy6AOuBUcBVQXZuZgY8BJS4++/qPUoRkTSrXrjePuJ0WmdQ4VqTWoPe3SvNbCwwG2gCTHP3YjMbE1s+xczaA0VAa6DKzMYDPYDewNeAZWa2OLbL77v7rKTPREQkRbKhcK1JoAumYsE8q9pjU+K+fp/oKZ3q5pD4HL+ISMbLpsK1JroyVkQkgWwrXGuioBcRiXOgypn8SimT/pFdhWtNFPQiIjHrtuzi5icXs+DdrYzseyI/G9kzawrXmijoRUT4/8LVgYlX9GVkv+wqXGuioBeRnPZR7DNcp2d54VoTBb2I5Kw3125lfOFiyrbuyvrCtSYKehHJOWEsXGuioBeRnFK2NXqFa9gK15oo6EUkZ8xYsoEf/HUZ7uErXGuioBeR0Ptoz35+MqOY6W+up39+GyaN6he6wrUmCnoRCbX4wrXg/O7c+PlwFq41UdCLSCjFF67tW7fgyRsGEekc3sK1Jgp6EQmd+MJ1RN8TuSMHCteaKOhFJFTiC9d7rujDJf0SvbFublHQi0go7NhbyY+fXf5x4Trxin7kH5c7hWtNFPQikvUWrd1KQY4XrjVR0ItI1jpQ5fzvK6VMVOFaIwW9iGQlFa7BKehFJOuocK0bBb2IZA0VrvWjoBeRrBBfuI47vzvjVLgGpqAXkYxWvXB94oZBnKnCtU4C/To0s2FmttLMSs3s1gTLTzWz181sr5ndUpdtRUQOZ/223Vw5dR6/ffltLurVgVkF5yrk66HWI3ozawJMBoYCZcACM5vh7iviVtsCjANG1mNbEZFPeG7JBr6vwjUpgpy6GQCUuvtqADMrBEYAH4e1u28ENprZRXXdVkQk3o69lfzk2WKeebOMfvltmKTCtcGCBH0esC7ufhkwMOD+A29rZqOB0QD5+fkBdy8iYbJo7VbGP7GYdVtUuCZTkKC3BI95wP0H3tbdpwJTASKRSND9i0gIHKhy7v9XKff8XYVrKgQJ+jKgU9z9jsCGgPtvyLYikgPWb9vNTYWLeePdLXy5T/QK12OO1BWuyRQk6BcA3c2sC7AeGAVcFXD/DdlWREIuvnD93Vf7cEm/PMwSnQiQhqg16N290szGArOBJsA0dy82szGx5VPMrD1QBLQGqsxsPNDD3bcn2jZFcxGRLKHCtXGZe+adDo9EIl5UVJTuYYhICsQXrmM/r8I1WcxsobtHEi3TlbEi0ihUuKaPgl5EUm79tt3c9MRi3lizhS/1OZE7Vbg2KgW9iKTUzKUb+P70ZVSpcE0bBb2IpIQK18yhoBeRpFu8bhsFhYuiV7h+/mRuPL87R6hwTRsFvYgkTfXCtXD0IAZ0UeGabgp6EUkKFa6ZS0EvIg12sHA9UOX89it9uLS/CtdMoqAXkXrbsbeSn84o5umFZfTt1IZJo/py0nFHpXtYUo2CXkTqRYVr9lDQi0idqHDNPgp6EQlsw7bdjFfhmnUU9CISyPNLy7lt+lIVrllIQS8iNdqxt5LbZxTzlArXrKWgF5HDii9cb/z8yYxT4ZqVFPQi8gkHqpwp/17FPS+/zQkqXLOegl5EDrEhdoXr/DVbuLh3B35+SS8VrllOQS8iH1PhGk4KehFhZ+wKVxWu4aSgF8lxS2KF63tbdjH2vJMpuECFa9go6EVy1CcK12+excCux6V7WJICCnqRHKTCNbcE+vvMzIaZ2UozKzWzWxMsNzO7N7Z8qZn1j1t2k5kVm9lyM3vczFokcwIiUjfPLy1n2MRXWb6+gt98pQ/3XdlPIR9ytQa9mTUBJgMXAj2AK82sR7XVLgS6x26jgftj2+YB44CIu/cEmgCjkjZ6EQls595KvvPUEr79lzfp0vZonh93Lpef0VGvqskBQU7dDABK3X01gJkVAiOAFXHrjAAedncH5plZGzPrEPccR5rZfqAlsCFpoxeRQFS45rYgQZ8HrIu7XwYMDLBOnrsXmdlvgLXAbuAld38p0ZOY2Wiifw2Qn58fbPQiUqP4wrVdq+YqXHNUkF/pif6u8yDrmNmxRI/2uwAnAkeZ2dWJnsTdp7p7xN0jbdu2DTAsEanJhm27ueqBefx69kq+2LM9LxQMUcjnqCBH9GVAp7j7Hfnk6ZfDrXMBsMbdNwGY2XTgbODR+g5YRGo3a1k5t01fRuWBKn7zlT5cpitcc1qQoF8AdDezLsB6omXqVdXWmQGMjZ2/HwhUuHu5ma0FzjKzlkRP3ZwPFCVt9CJyiPgrXPt0asOkK/rS+Xhd4Zrrag16d680s7HAbKKvmpnm7sVmNia2fAowCxgOlAK7gGtjy+ab2dPAm0AlsAiYmoqJiOQ6Fa5yOBZ9oUxmiUQiXlSkA3+RIKoXrvdc0Vfn4nOQmS1090iiZboyViSLxV/helHvDtw1shfHtNTFT3IoBb1IljpYuO4/UMWvL++ti5/ksBT0Illm595Kbn+umCeLyujT8RgmjeqnwlVqpKAXySJL1m1j/BOLeffDnSpcJTAFvUgWOFDl/OHVVfzuJV3hKnWnoBfJcOUV0cJ13moVrlI/CnqRDPbCsnJuVeEqDaSgF8lAKlwlmRT0Ihlmadk2Cgqjheu3z+vG+AtOUeEqDaKgF8kQ8YVr21bNefybZ3GWCldJAgW9SAY4pHDt1YG7LlHhKsmjoBdJs/jC9e7Le/MVFa6SZAp6kTTZubeSnz23gieK1tGn4zFMHNWPLipcJQUU9CJpoMJVGpOCXqQRqXCVdFDQizQSFa6SLgp6kUagwlXSSUEvkkIqXCUTKOhFUiS+cP3W57px01AVrpIeCnqRJKuqcv7w6mp++9JK2rZqzl+uP4tB3VS4Svoo6EWSqLxiNzc/sYTXV3/I8F7tueuSXrRp2Szdw5IcF+jvSDMbZmYrzazUzG5NsNzM7N7Y8qVm1j9uWRsze9rM3jKzEjMblMwJiGSKF5aVM2ziaywp28bdl/dm8lX9FfKSEWo9ojezJsBkYChQBiwwsxnuviJutQuB7rHbQOD+2L8Ak4AX3f1yM2sGtEzi+EXSbte+aOFauGAdvWNvKazCVTJJkFM3A4BSd18NYGaFwAggPuhHAA+7uwPzYkfxHYCdwBDg6wDuvg/Yl7zhi6TX0rJtjC9czBoVrpLBggR9HrAu7n4Z/3+0XtM6eUAlsAn4o5n1ARYCBe6+s94jFskAKlwlmwQ59Eh0VYcHXKcp0B+43937ET3C/8Q5fgAzG21mRWZWtGnTpgDDEkmP8ord/NeD8/nVi2/xhdNP4IWCcxXyktGCHNGXAZ3i7ncENgRcx4Eyd58fe/xpDhP07j4VmAoQiUSq/yIRyQgvLi/ne8/ErnC9rDdfiegKV8l8QYJ+AdDdzLoA64FRwFXV1pkBjI2dvx8IVLh7OYCZrTOzz7j7SuB8Dj23L5IVVLhKNqs16N290szGArOBJsA0dy82szGx5VOAWcBwoBTYBVwbt4sbgcdir7hZXW2ZSMZbVlZBQeEi1ny4k//5XDduuuAUmjVV4SrZw6IvlMkskUjEi4qK0j0MyXFVVc7U16KF63FHNeeeK/rqXLxkLDNb6O6RRMt0ZaxIAuUVu5nw5BL+s+pDLuzZnl9cqitcJXsp6EWqUeEqYaOgF4lR4SphpaAXQYWrhJuCXnJa9cL1sesHcna349M9LJGkUtBLznq/Yg83P7lYhauEnoJectKLy9/n1ulL2bu/il9d1ouvRjqpcJXQUtBLTtm1r5I7Zq7g8TfW0SvvGCaN6kvXtkene1giKaWgl5yhwlVylYJeQk+Fq+Q6Bb2E2vsVe5jw1GLmlqpwldyloJfQUuEqEqWgl9BR4SpyKAW9hMry9RWMK1zEms07GfPZbtw8VIWriIJeQqGqynngtdX8RoWryCco6CXrxReuw06PFq7HHqXCVeQgBb1ktdnF7/O9Z1S4itREQS9ZKVq4lvD4G2tVuIrUQkEvWSe+cL3hs12ZMPQzKlxFaqCgl6zxicL1uoGcfbIKV5HaKOglK6hwFak/Bb1kvPjC9ZeX9uKKM1W4itRFoBObZjbMzFaaWamZ3ZpguZnZvbHlS82sf7XlTcxskZnNTNbAJfx27avktunLuOGRhXQ6tiUzxw1m1IB8hbxIHdV6RG9mTYDJwFCgDFhgZjPcfUXcahcC3WO3gcD9sX8PKgBKgNZJGreEnApXkeQJ8n/OAKDU3Ve7+z6gEBhRbZ0RwMMeNQ9oY2YdAMysI3AR8GASxy0hVVXlTH11FZf871x27q3ksesGctuFpynkRRogyDn6PGBd3P0yDj1aP9w6eUA5MBH4LtCqpicxs9HAaID8/PwAw5Kw+WD7HiY8uYQ5pZv54ukn8MtLe6twFUmCIEGf6ISoB1nHzC4GNrr7QjP7XE1P4u5TgakAkUik+v4l5FS4iqROkKAvAzrF3e8IbAi4zuXAl81sONACaG1mj7r71fUfsoRJ/BWuPfNaM2lUP7rpCleRpApy4nMB0N3MuphZM2AUMKPaOjOAa2KvvjkLqHD3cne/zd07unvn2Hb/VMjLQcvXV3DxfXMoXLCWGz7blen/c45CXiQFaj2id/dKMxsLzAaaANPcvdjMxsSWTwFmAcOBUmAXcG3qhizZrqrKeXDOan49eyWfPqqZrnAVSTFzz7zT4ZFIxIuKitI9DEkBFa4iqWFmC909kmiZroyVRvNSrHDds7+KX1zai1EqXEUahYJeUm73vgPc8fwK/jJfhatIOijoJaV0hatI+inoJSWqqpyH5qzh7tlvqXAVSTMFvSSdCleRzKKgl6Q6WLju3n+Auy7pxZUDVLiKpJuCXpKieuE68Yp+nNxOhatIJlDQS4MtX19BQeEiVm3ayQ1DujLhCypcRTKJgl7q7ROF6/UDOUeFq0jGUdBLvXywfQ+3PLWE197ZzBd6nMCvLlPhKpKpFPRSZy+v+IDvPr1EhatIllDQS2C79x3gzudX8Nj8tZx+YvQKVxWuIplPQS+BxBeuo4d0ZcIXTqF50ybpHpaIBKCglxpVL1wfvW4gg7urcBXJJgp6OayN2/cwQYWrSNZT0EtCKlxFwkNBL4dQ4SoSPgp6+VjxhgoKChdTunGHCleREFHQC1VVzrS5a7j7xZW0aXmECleRkFHQ57j4wnVorHD9tApXkVBR0OcwFa4iuUFBn4N27zvAz2et4NF5KlxFckGg95I1s2FmttLMSs3s1gTLzczujS1famb9Y493MrNXzKzEzIrNrCDZE5C6Kd5QwZd+P4dH561l9JCuTP/W2Qp5kZCr9YjezJoAk4GhQBmwwMxmuPuKuNUuBLrHbgOB+2P/VgIT3P1NM2sFLDSzl6ttK41AhatI7gpy6mYAUOruqwHMrBAYAcSH9QjgYXd3YJ6ZtTGzDu5eDpQDuPtHZlYC5FXbVlJMhatIbgsS9HnAurj7ZUSP1mtbJ49YyAOYWWegHzA/0ZOY2WhgNEB+fn6AYUkQL6/4gO89s5Rd+yr5+SU9uWpAvgpXkRwTJOgTpYLXZR0zOxp4Bhjv7tsTPYm7TwWmAkQiker7lzqKL1x7dGjNvVf25eR2rdI9LBFJgyBBXwZ0irvfEdgQdB0zO4JoyD/m7tPrP1QJasWG7YwrXETpxh1889wu3PLFz+gKV5EcFiToFwDdzawLsB4YBVxVbZ0ZwNjY+fuBQIW7l1v0HMFDQIm7/y6J45YEqheuj1w3gHO7t033sEQkzWoNenevNLOxwGygCTDN3YvNbExs+RRgFjAcKAV2AdfGNj8H+BqwzMwWxx77vrvPSuos5JDC9YLTTuDuy1W4ikiURV8ok1kikYgXFRWlexhZ4+8rPuC7scL1Rxf3UOEqkoPMbKG7RxIt05WxWUyFq4gEoaDPUipcRSQoBX2WUeEqInWloM8iKlxFpD5CE/Tuzp3Pl3D+ae04u1v43sPlHyUf8J2no4XrnSN78l8DVbiKSDChCfqK3ft55a2NTJu7hhuGdOPmoafQrGmgN+fMaLv3HeCuWSU8Mu89TuvQmvtUuIpIHWV/Esa0admMmeMGM+rMTkz59youu/8/rNq0I93DapAVG7bzpd/P4ZF573H94C787dtnK+RFpM5CE/QALZs15ReX9mbK1WewbusuLr53Do+/sZZMvFagJlVVzoOvrWbk5LlU7N7Pw98YwA8v7qFX1YhIvYTm1E28YT3b07dTGyY8tZjbpi/jXys38stLe3NsFhSXG7fv4Zanl/Lq25tUuIpIUoTqiD5e+2Na8Mg3BvKD4afxz7c2MmzSq8x5Z3O6h1Wjf5R8wLBJr/HGmg+5c2RPHrjmDIW8iDRYaIMe4FOfMr45pCt//dY5HN28KVc/NJ+7ZpWwt/JAuod2iD37D/Cjvy3nuj8XcULrFsy8cTBXn3WSXlUjIkkRylM31fXMO4aZN57Lz2etYOqrq5nzzuaMebuAkvLtjHt8Ee9s3MH1g7vwnWG6wlVEkivUR/TxjmzWhDtH9uKBayK8v30PF983h0fnvZe2oraqynlozhpG/H4u21S4ikgK5cQRfbyhPU6gT8dzmfDUEn74t+X8a+VGfnVZb447unmjjWHjR3u45amDhWu7Rn9+EcktOXNEH69d6xb8+doB/OjiHrz69maGTXqNV9/e1CjP/Y+SD7hw4mvMX/0hd4zsyQPXRBTyIpJSORn0EC1qrxvchWfHnkObI4/gmmlv8LPnVrBnf2qK2j37D/DjZ6OFa7vWLXh+3GC+psJVRBpBzp26qe60Dq157sbB/GJWCdPmruE/qzZz75X9OOWE5BW18YXrdYO78F0VriLSiHL2iD5eiyOacPuInvzx62eyecdevnTfHP78n3cbXNQmKlx/pMJVRBqZgj7Oeae244WCIQzqdhw/mVHMN/60gM079tZrXxs/2sPX/7SAO2auYMgpx/NiwbkMOUXvGy8ijU9BX03bVs3549fP5PYvn87cVR8ybOKrvLJyY532ocJVRDKJgj4BM+O/z+7Mc2MHc/zRzbn2jwv46YziWova6oXrzBtVuIpI+uV8GVuTz7Rvxd++fQ53v7iSaXPX8PqqD5l0ZV9Obd/6E+uWlG+noHARb3+gwlVEMkugI3ozG2ZmK82s1MxuTbDczOze2PKlZtY/6LaZrsURTfjxl3rwp2vP5MOd+/jy7+cybc4aqqqiRW1VlTNtzhpGTJ7L1l0qXEUk81htrywxsybA28BQoAxYAFzp7ivi1hkO3AgMBwYCk9x9YJBtE4lEIl5UVFTvSaXKhzv28r1nlvL3ko0MOaUttw47lV+9+Bb/fnsT55/ajrsv1xWuIpIeZrbQ3SOJlgU5dTMAKHX31bGdFQIjgPiwHgE87NHfGvPMrI2ZdQA6B9g2axx3dHMeuCbCo/PXcufMFQy/9zWaN/0Ud4zsydX6DFcRyVBBgj4PWBd3v4zoUXtt6+QF3BYAMxsNjAbIz88PMKz0MDO+dtZJDOr6af70n3f570Gd6Z7Ei6tERJItyDn6RIep1c/3HG6dINtGH3Sf6u4Rd4+0bZv5rzc/uV0r7hzZSyEvIhkvyBF9GdAp7n5HYEPAdZoF2FZERFIoyBH9AqC7mXUxs2bAKGBGtXVmANfEXn1zFlDh7uUBtxURkRSq9Yje3SvNbCwwG2gCTHP3YjMbE1s+BZhF9BU3pcAu4Nqatk3JTEREJKFaX16ZDpn68koRkUxV08sr9RYIIiIhp6AXEQk5Bb2ISMgp6EVEQi4jy1gz2wS8l+5xBHA8sDndg0ihMM9Pc8teYZ5fQ+Z2krsnvNo0I4M+W5hZ0eFa7jAI8/w0t+wV5vmlam46dSMiEnIKehGRkFPQN8zUdA8gxcI8P80te4V5fimZm87Ri4iEnI7oRURCTkEvIhJyCvrDCPMHotd3bmbWycxeMbMSMys2s4LGH33tGvK9iy1vYmaLzGxm4406mAb+XLYxs6fN7K3Y93BQ446+Zg2c202xn8nlZva4mbVo3NHXLsD8TjWz181sr5ndUpdta+XuulW7EX1L5VVAV6IfnrIE6FFtneHAC0Q/RessYH7QbbN4bh2A/rGvWxH94PeMmVtD5xe3/GbgL8DMdM8nmXMD/gxcH/u6GdAm3XNK0s9lHrAGODJ2/0ng6+meUz3m1w44E/g5cEtdtq3tpiP6xD7+QHR33wcc/FDzeB9/ILq7zwMOfiB6kG3Tqd5zc/dyd38TwN0/AkqI/k+WSRryvcPMOgIXAQ825qADqvfczKw1MAR4CMDd97n7tkYce20a9H0j+tkaR5pZU6AlmfdJdrXOz903uvsCYH9dt62Ngj6xw33YeZB1gmybTg2Z28fMrDPQD5if/CE2SEPnNxH4LlCVovE1REPm1hXYBPwxdlrqQTM7KpWDraN6z83d1wO/AdYC5UQ/4e6lFI61PhqSCw3OFAV9Yo3ygehp0pC5RReaHQ08A4x39+1JHFsy1Ht+ZnYxsNHdFyZ/WEnRkO9dU6A/cL+79wN2ApnUHzXk+3Ys0SPcLsCJwFFmdnWSx9dQDcmFBmeKgj6xhnwgepBt06khc8PMjiAa8o+5+/QUjrO+GjK/c4Avm9m7RP88/ryZPZq6odZZQ38uy9z94F9gTxMN/kzRkLldAKxx903uvh+YDpydwrHWR0NyoeGZku6SIhNvRI9+VhM9QjhYfpxebZ2LOLQYeiPotlk8NwMeBiamex6pmF+1dT5H5pWxDZob8BrwmdjXPwV+ne45JennciBQTPTcvBEtnW9M95zqOr+4dX/KoWVsgzMl7f8BMvVGtOF/m2jb/YPYY2OAMbGvDZgcW74MiNS0bSbd6js3YDDRPxmXAotjt+Hpnk8yv3dx+8i4oE/Cz2VfoCj2/fsbcGy655PEud0OvAUsBx4Bmqd7PvWYX3uiR+/bgW2xr1sfbtu63PQWCCIiIadz9CIiIaegFxEJOQW9iEjIKehFREJOQS8iEnIKehGRkFPQi4iE3P8B9t8IA6SckWcAAAAASUVORK5CYII=",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from scipy.optimize import minimize, root\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "print()\n",
    "print(\"Part C: Leave-One-Out Error\")\n",
    "\n",
    "times = [1900, 1910, 1920,1930,1940,1950,1960,1970,1980,1990,2000,2010]\n",
    "times = [i for i in range(12)]\n",
    "b = [75.995, 91.972, 105.711, 123.203, 131.669, 150.697, 179.323, 203.212, 226.505, \n",
    "249.633, 281.422, 308.746]\n",
    "x_vals = []\n",
    "\n",
    "for d in range(len(b)):\n",
    "    times = [times[i] if i != d else 0 for i in range(12)]\n",
    "    b = [b[i] if i != d else 0 for i in range(12)]\n",
    "    \n",
    "    B = A(times,12)\n",
    "    x = cp.Variable(12)\n",
    "    cost = cp.norm(B@x - b, p=\"inf\")\n",
    "    prob = cp.Problem(cp.Minimize(cost))\n",
    "    prob.solve()\n",
    "\n",
    "    x_vals.append(x.value)\n",
    "\n",
    "\n",
    "l_vals = np.linspace(-0.01, 0.1,2000)\n",
    "n_vals = []\n",
    "for l in l_vals:\n",
    "    n = 0\n",
    "    for x in x_vals:\n",
    "        n += abs(np.linalg.norm(B@x-b, ord=2)**2 + l*np.linalg.norm(x,ord=1))\n",
    "    n_vals.append(n/2000)\n",
    "\n",
    "plt.plot(l_vals,n_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Problem 2, Part a\n",
      "\n",
      "   direc: array([[-2.70827464e-01,  4.05727538e-01,  1.01960361e+00],\n",
      "       [-4.80805233e-02,  4.79662872e-01, -4.97176916e+00],\n",
      "       [-4.58989348e-04,  3.68191841e-03, -5.43423745e-02]])\n",
      "     fun: 0.08055433729963207\n",
      " message: 'Optimization terminated successfully.'\n",
      "    nfev: 251\n",
      "     nit: 6\n",
      "  status: 0\n",
      " success: True\n",
      "       x: array([ 0.47378861,  2.24119794, -1.45718831])\n"
     ]
    }
   ],
   "source": [
    "from scipy.optimize import minimize, root\n",
    "\n",
    "\n",
    "print(\"-------\")\n",
    "print(\"Problem 2, Part a\")\n",
    "print()\n",
    "\n",
    "def f(theta):\n",
    "    x_1 = theta[0]*theta[2] + theta[1]*u[0]\n",
    "    x_2 = theta[0]*x_1 + theta[1]*u[1]\n",
    "    x_3 = theta[0]*x_2 + theta[1]*u[2]\n",
    "    x_4 = theta[0]*x_3 + theta[1]*u[3]\n",
    "    x = np.array([x_1-y[0],x_2-y[1],x_3-y[2],x_4-y[3]])\n",
    "    \n",
    "    return np.linalg.norm(x)\n",
    "\n",
    "\n",
    "u = np.array([5, 2, 0, 1])\n",
    "y  = np.array([10.5, 9.5, 4.51, 4.3])\n",
    "\n",
    "theta = minimize(f,[1,1,1],method='powell')\n",
    "print(theta)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From above yields values:\n",
    "\n",
    "a = 0.474\n",
    "\n",
    "b = 2.24\n",
    "\n",
    "x_0 = -1.457"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Problem 3, Part c\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\davee\\AppData\\Local\\Temp\\ipykernel_2208\\3975664844.py:31: RuntimeWarning: invalid value encountered in matmul\n",
      "  g = np.array([(p@x_0) for p in phi])\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       " active_mask: array([0, 0, 0, 0, 0])\n",
       "        cost: 26451.25794823645\n",
       "         fun: array([150.71218431, 171.65570494,  10.97269659,  16.45682041,\n",
       "        18.20564063])\n",
       "        grad: array([    60.66388159,   -969.03103578, -15578.03840278, -20613.32322988,\n",
       "        30061.29259625])\n",
       "         jac: array([[ 0.00000000e+00,  0.00000000e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00],\n",
       "       [ 3.29129554e-01, -5.25744052e+00,  0.00000000e+00,\n",
       "         0.00000000e+00,  0.00000000e+00],\n",
       "       [-4.27625994e-02,  6.83083061e-01,  9.03915766e+03,\n",
       "         4.18782425e+04, -2.49437528e+05],\n",
       "       [-2.34035843e-01,  3.73843799e+00, -2.09313166e+05,\n",
       "        -1.19230263e+05,  2.15028329e+05],\n",
       "       [ 4.66208418e-01, -7.44711188e+00,  1.82903051e+05,\n",
       "         8.14044661e+04, -4.23835118e+04]])\n",
       "     message: '`xtol` termination condition is satisfied.'\n",
       "        nfev: 85\n",
       "        njev: None\n",
       "  optimality: 30061.292596247047\n",
       "      status: 3\n",
       "     success: True\n",
       "           x: array([ 4.53060116e+01,  2.83627505e+00, -9.22125366e-05,  1.26331439e-04,\n",
       "       -2.34619586e-05])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.optimize import least_squares\n",
    "\n",
    "print(\"-------\")\n",
    "print(\"Problem 3, Part c\")\n",
    "print()\n",
    "\n",
    "import scipy.linalg as spla\n",
    "from scipy.optimize import minimize, root\n",
    "\n",
    "y = np.array([[88.35, 7.3, 2.3, 0.4, 1.75], \n",
    "[76.4, 15.6, 4.5, 0.7, 2.8], \n",
    "[65.1, 23.1, 5.3, 1.1, 5.8], \n",
    "[50.4, 32.9, 6.0, 1.5, 9.3], \n",
    "[37.5, 42.7, 6.0, 1.9, 12.0], \n",
    "[25.9, 49.1, 5.9, 2.2, 17.0], \n",
    "[14.0, 57.4, 5.1, 2.6, 21.0], \n",
    "[4.5, 63.1, 3.8, 2.9, 25.7]])\n",
    "t = [1230.0, 3060.0, 4920.0, 7800.0, 10680.0, 15030.0, 22620.0, 36420.0]\n",
    "x_0 = np.array([100,0,0,0,0])\n",
    "\n",
    "def A_matrix(theta):\n",
    "    A = [[-1*(theta[0] + theta[1]),0,0,0,0], \n",
    "    [theta[0], 0,0,0,0],\n",
    "    [theta[1], 0,-1*(theta[2]+theta[3]),0,theta[4]],\n",
    "    [0,0, theta[2],0,0],\n",
    "    [0,0,theta[3],0,-1*theta[4]]]\n",
    "    return np.array(A)\n",
    "\n",
    "def exp(theta):\n",
    "    phi = np.array([spla.expm(A_matrix(theta) * t_i) for t_i in t])\n",
    "    g = np.array([(p@x_0) for p in phi])\n",
    "    C = [np.linalg.norm([y[i][j] - g[i][j] for i in range(len(g))],ord=2) for j in range(len(g[0]))]\n",
    "    return C\n",
    "\n",
    "least_squares(exp, x_0, method=\"lm\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unfortunately I could not get the method above to work properly. I tried many, many different approaches with both cvxpy and scipy. I do believe the idea is formulated correctly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------\n",
      "Problem 6\n",
      "\n",
      "0: Train Error: 2.7274 --- Test Error: 1.2401\n",
      "Bias Squared:  0.19801549601093224\n",
      "Variance:  0.0\n",
      "1: Train Error: 0.6585 --- Test Error: 0.2976\n",
      "Bias Squared:  0.001163304189508706\n",
      "Variance:  0.3963088320289178\n",
      "2: Train Error: 0.4158 --- Test Error: 0.2298\n",
      "Bias Squared:  0.0007919537997041267\n",
      "Variance:  0.3371060043919306\n",
      "3: Train Error: 0.3952 --- Test Error: 0.2384\n",
      "Bias Squared:  0.0007828596560050898\n",
      "Variance:  0.3247330664855823\n",
      "4: Train Error: 0.3805 --- Test Error: 0.2301\n",
      "Bias Squared:  0.0009471387670305198\n",
      "Variance:  0.3142158502235272\n",
      "5: Train Error: 0.3638 --- Test Error: 0.2424\n",
      "Bias Squared:  0.0010864864889515726\n",
      "Variance:  0.3136025367619151\n",
      "6: Train Error: 0.3554 --- Test Error: 0.2571\n",
      "Bias Squared:  0.0011387237654304086\n",
      "Variance:  0.3483651399929112\n",
      "7: Train Error: 0.3408 --- Test Error: 0.2786\n",
      "Bias Squared:  0.0014420598716243856\n",
      "Variance:  0.37932627494009286\n",
      "8: Train Error: 0.3297 --- Test Error: 0.291\n",
      "Bias Squared:  0.0017546085163993293\n",
      "Variance:  0.2723083870700254\n",
      "9: Train Error: 0.3054 --- Test Error: 0.2884\n",
      "Bias Squared:  0.0018141047197234646\n",
      "Variance:  0.30400304244684245\n",
      "10: Train Error: 0.2904 --- Test Error: 0.3223\n",
      "Bias Squared:  0.0020586959958436824\n",
      "Variance:  0.43331787648617887\n",
      "11: Train Error: 0.2751 --- Test Error: 0.4592\n",
      "Bias Squared:  0.005569885588060401\n",
      "Variance:  0.3488967409636879\n",
      "12: Train Error: 0.2523 --- Test Error: 0.7376\n",
      "Bias Squared:  0.021793266499579717\n",
      "Variance:  0.2758874252743809\n",
      "13: Train Error: 0.2355 --- Test Error: 1.4645\n",
      "Bias Squared:  0.11712281046901553\n",
      "Variance:  0.4135953253855161\n",
      "14: Train Error: 0.2149 --- Test Error: 2.146\n",
      "Bias Squared:  0.26808290088390496\n",
      "Variance:  0.5968883563716227\n",
      "15: Train Error: 0.1851 --- Test Error: 6.333\n",
      "Bias Squared:  2.5075783081567105\n",
      "Variance:  4.673603326199224\n",
      "16: Train Error: 0.1583 --- Test Error: 10.8932\n",
      "Bias Squared:  7.594660110406357\n",
      "Variance:  2.4527221837505437\n",
      "17: Train Error: 0.125 --- Test Error: 33.0089\n",
      "Bias Squared:  68.10579388133998\n",
      "Variance:  3.4164534538165063\n",
      "18: Train Error: 0.0793 --- Test Error: 179.3388\n",
      "Bias Squared:  2012.732342063327\n",
      "Variance:  31.930510787996877\n",
      "19: Train Error: 0.0 --- Test Error: 373.7144\n",
      "Bias Squared:  8675.168467887366\n",
      "Variance:  135.02620715457147\n"
     ]
    }
   ],
   "source": [
    "print(\"-------\")\n",
    "print(\"Problem 6\")\n",
    "print()\n",
    "\n",
    "# set up function and data\n",
    "f = lambda t: math.exp(-1*t)\n",
    "alt_f = lambda t: math.sin(12*t)\n",
    "t = np.random.uniform(-1,1,25)\n",
    "# partition data into training and validation sets\n",
    "ry = [f(t[i]) for i in range(25)]\n",
    "\n",
    "def A(times,m2=2,m1=0):\n",
    "    B =  [[t**i for i in range(m1,m2)] for t in times]\n",
    "    return np.array(B)\n",
    "\n",
    "def bias(x_hat, x):\n",
    "    return np.mean(x_hat-x,axis=0)\n",
    "\n",
    "def exp_error(x_hat,x):\n",
    "    return np.mean((x-x_hat)**2, axis=0)\n",
    "\n",
    "\n",
    "for N in range(1,21):\n",
    "    train_error = 0\n",
    "    test_error = 0\n",
    "    errors = []\n",
    "    bias_squared = 0\n",
    "\n",
    "    for k in range(1000):\n",
    "        e = np.random.normal(0,.1,25)\n",
    "        y = [f(t[i]) + e[i] for i in range(25)]\n",
    "        y_train, y_test = y[:20], y[20:]\n",
    "        t_train, t_test = t[:20], t[20:]\n",
    "\n",
    "        B_train = A(t_train,N)\n",
    "        theta, err = np.linalg.lstsq(B_train,y_train, rcond=1)[:2]\n",
    "\n",
    "        B_test = A(t_test, N)\n",
    "        train_error += np.linalg.norm(B_train@theta - y_train)\n",
    "\n",
    "        pred_val = B_test@theta\n",
    "        test_error += np.linalg.norm(pred_val - y_test)\n",
    "        errors.append(test_error)\n",
    "\n",
    "        bias_squared += bias(pred_val, ry[20:])**2\n",
    "\n",
    "    print(str(N-1) + \": Train Error: \" + str(round(train_error/1000,4)) + \" --- Test Error: \" + str(round(test_error/1000,4)))\n",
    "    print(\"Bias Squared: \", bias_squared/1000)\n",
    "    print(\"Variance: \", np.std(pred_val))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.12 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e3f058019491b5a987a146e2cbdf9c3254eb932582dd8a90e9955176c2995d91"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
